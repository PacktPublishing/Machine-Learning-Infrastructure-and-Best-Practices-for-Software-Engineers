{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1u50IzR1serItKdQZfUJI_aFW3G--M9aP","authorship_tag":"ABX9TyOkLbNcPLnybbWBIfDrw+i1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Sentence piece tokenizer\n","\n","This notebook is a demonstration of how to use the sentence piece tokenizer. The notebook is based on the official documentation notebook available here: https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n","\n","The original examples is under Apache License 2.0. The changes in this workbook include:\n","1) different training set, as in the rest of the book\n","2) different example, as in the rest of the book"],"metadata":{"id":"s59zvyAhBht-"}},{"cell_type":"code","source":["# installing the sentence piece tokenizer\n","!pip install -q sentencepiece"],"metadata":{"id":"-K0di4bdB8ft","executionInfo":{"status":"ok","timestamp":1683360248073,"user_tz":-120,"elapsed":6464,"user":{"displayName":"Miroslaw Staron","userId":"03361447941535209117"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b9a35549-f4c5-4c9c-e99b-c4ef7bf8404f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZRTt9xMiBhAg","executionInfo":{"status":"ok","timestamp":1683360258231,"user_tz":-120,"elapsed":2307,"user":{"displayName":"Miroslaw Staron","userId":"03361447941535209117"}},"outputId":"452e039a-970f-4e89-b4d9-cc65d0760cdb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["import sentencepiece as spm\n","\n","# this statement trains the tokenizer\n","spm.SentencePieceTrainer.train('--input=\"/content/drive/MyDrive/ds/cs_dos/nx_icmp_checksum_compute.c\" --model_prefix=m --vocab_size=200')\n","\n","# makes segmenter instance and loads the model file (m.model)\n","sp = spm.SentencePieceProcessor()\n","sp.load('m.model')"]},{"cell_type":"code","source":["strCProgram = '''\n","int main(int argc, void **argc)\n","{\n","  printf(\"%s\", \"Hello World\\n\");\n","  return 0; \n","}\n","'''"],"metadata":{"id":"g7In3d3KF2lb","executionInfo":{"status":"ok","timestamp":1683360303059,"user_tz":-120,"elapsed":514,"user":{"displayName":"Miroslaw Staron","userId":"03361447941535209117"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# encode: text => id\n","print(sp.encode_as_pieces(strCProgram))\n","print(sp.encode_as_ids(strCProgram))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KX92n07xHA8h","executionInfo":{"status":"ok","timestamp":1683360322204,"user_tz":-120,"elapsed":410,"user":{"displayName":"Miroslaw Staron","userId":"03361447941535209117"}},"outputId":"1f0b2e51-aaea-41ed-ec44-882234f0ca53"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['▁in', 't', '▁', 'm', 'a', 'in', '(', 'in', 't', '▁a', 'r', 'g', 'c', ',', '▁', 'v', 'o', 'i', 'd', '▁*', '*', 'a', 'r', 'g', 'c', ')', '▁', '{', '▁', 'p', 'r', 'in', 't', 'f', '(', '\"', '%', 's', '\"', ',', '▁', '\"', 'H', 'e', 'll', 'o', '▁', 'W', 'o', 'r', 'l', 'd', '▁', '\"', ')', ';', '▁', 're', 't', 'u', 'r', 'n', '▁0', ';', '▁', '}']\n","[50, 25, 3, 79, 38, 75, 20, 75, 25, 42, 60, 116, 32, 96, 3, 182, 45, 19, 66, 6, 58, 38, 60, 116, 32, 10, 3, 61, 3, 34, 60, 75, 25, 82, 20, 120, 0, 12, 120, 96, 3, 120, 198, 30, 136, 45, 3, 70, 45, 60, 44, 66, 3, 120, 10, 11, 3, 92, 25, 31, 60, 43, 107, 11, 3, 62]\n"]}]},{"cell_type":"code","source":["# decode: id => text\n","print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n","print(sp.decode_ids([18, 135, 12]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnUf243mHs-n","executionInfo":{"status":"ok","timestamp":1683226583923,"user_tz":-120,"elapsed":233,"user":{"displayName":"Miroslaw Staron","userId":"03361447941535209117"}},"outputId":"4f23a96d-8578-48d4-b03a-2b9dacc52766"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["▁This▁is a▁test\n","This\n"]}]},{"cell_type":"code","source":["# returns vocab size\n","print(sp.get_piece_size())\n","\n","# id <=> piece conversion\n","print(sp.id_to_piece(18))\n","print(sp.piece_to_id('T'))\n","\n","# returns 0 for unknown tokens (we can change the id for UNK)\n","print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n","\n","# , ,  are defined by default. Their ids are (0, 1, 2)\n","#  and  are defined as 'control' symbol.\n","for id in range(3):\n","  print(sp.id_to_piece(id), sp.is_control(id))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qOAQCgxhE4e2","executionInfo":{"status":"ok","timestamp":1683226619070,"user_tz":-120,"elapsed":3,"user":{"displayName":"Miroslaw Staron","userId":"03361447941535209117"}},"outputId":"a84a0cb4-950a-40d8-ad24-3e10cd1327c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["200\n","T\n","18\n","0\n","<unk> False\n","<s> True\n","</s> True\n"]}]}]}