{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M1oqh0F6W3ad"
   },
   "source": [
    "# Training a BERT model from scratch\n",
    "\n",
    "This demo illustrates the training of a BERT model from scratch. The model is trained on a small dataset of wolfSSL, which is an SSL library used in embedded software development. \n",
    "\n",
    "The notebook is based on the existing tutorial from Hugging Face [link](https://huggingface.co/blog/how-to-train).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aleXCJqVQPro"
   },
   "source": [
    "# Training a tokenizer\n",
    "\n",
    "The first step in training the model is to train the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have multuple GPUs, we can choose which one to use\n",
    "# if we have only one GPU, we can set it to 0\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just checking if CUDA is available on this computer\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMnymRDLe0hi",
    "outputId": "b7780f27-ce3b-4a3f-fcf5-ba11529b1d44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We use the standard BPE tokenizer for this workbook\n",
    "# it was described in the previous chapter of the book\n",
    "# when we discussed feature extraction\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = ['source_code_wolf_ssl.txt']\n",
    "\n",
    "print(f'Found {len(paths)} files')\n",
    "print(f'First file: {paths[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "print('Training tokenizer...')\n",
    "\n",
    "# Customize training\n",
    "# we use a large vocabulary size, but we could also do with ca. 10_000\n",
    "tokenizer.train(files=paths, \n",
    "                vocab_size=52_000, \n",
    "                min_frequency=2, \n",
    "                special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqYKX1XYyRI-",
    "outputId": "8e2f0316-455d-443b-ca8b-146bdd2e3a6a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# we give this model a catchy name - wolfBERTa\n",
    "# because it is a RoBERTa model trained on the WolfSSL source code\n",
    "token_dir = './wolfBERTa'\n",
    "\n",
    "if not os.path.exists(token_dir):\n",
    "  os.makedirs(token_dir)\n",
    "\n",
    "tokenizer.save_model('wolfBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9hQqVS_qZWg",
    "outputId": "0cfc92c7-10d2-4dec-93e1-2a6397dc8d33"
   },
   "outputs": [],
   "source": [
    "# finally, we can test the tokenizer\n",
    "tokenizer.encode(\"int main(int argc, void **argv)\").tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3hlRcyWsQjVq"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "Now, we can start preparing to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO5M3vrAhcuj"
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# let's make sure that the tokenizer does not provide more tokens than we expect\n",
    "# we expect 510 tokens, because we will use the BERT model\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTXXutqeDzPi"
   },
   "outputs": [],
   "source": [
    "# import the RoBERTa configuration\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "# initialize the configuration\n",
    "# please note that the vocab size is the same as the one in the tokenizer. \n",
    "# if it is not, we could get exceptions that the model and the tokenizer are not compatible\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-UsuK9Ps0H7",
    "outputId": "ec97bd11-0562-44f6-9663-aa7ae5fd4ea8"
   },
   "outputs": [],
   "source": [
    "# let's print the configuration\n",
    "# please note that there is more parameters than what we configured\n",
    "# this is because we use the default values for the rest of the parameters\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzMqR-dzF4Ro",
    "outputId": "b43302a1-723d-4126-eb82-3bcb2f3fc584",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing a Model From Scratch\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "# initialize the model\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "# let's print the number of parameters in the model\n",
    "print(model.num_parameters())\n",
    "\n",
    "# let's print the model\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset for training\n",
    "\n",
    "We use the datasets library from Hugging Face in order  to load the dataset. It allows us to work with larger datasets and in a more efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but before we actually train the model\n",
    "# we need to change the tokenizer to the one that we trained\n",
    "# and to make it compatible with the tokenizer that is expected by the model\n",
    "# so we read it from the file under a different tokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# initialize the tokenizer from the file\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"./wolfBERTa\", max_length=512)\n",
    "\n",
    "# please note that if we use a tokenizer that was trained before\n",
    "# the vanilla version of BPETokenizer, we will get an exception\n",
    "# that the BPE tokenizer is not collable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if we can change this to use the Dataset library instead of the transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "new_dataset = load_dataset(\"text\", data_files='./source_code_wolf_ssl.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's tokenize the dataset\n",
    "\n",
    "# num_proc is the argument to use all cores\n",
    "tokenized_dataset = new_dataset.map(lambda x: tokenizer(x[\"text\"]), num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTgWPa9Dipk2"
   },
   "outputs": [],
   "source": [
    "# training of the model requires a data collator\n",
    "# which creates a random set of tokens to mask\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpvnFFmZJD-N"
   },
   "outputs": [],
   "source": [
    "# now, we can train the model\n",
    "# by creating the trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wolfBERTa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "id": "VmaHZXzmkNtJ",
    "outputId": "000eb0e7-ce3a-460b-b972-3cea463bc2fe",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# here is where we train the model\n",
    "# which corresponds to the model.fit() method in Keras\n",
    "# which we used in the previous chapters\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-2hD_OLSa81"
   },
   "source": [
    "# Save the final model to hard drive\n",
    "\n",
    "Finally, we save the model to the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDNgPls7_l13",
    "outputId": "b86b8d70-2178-4a2e-bf09-ab417925698e"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./wolfBERTa\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The code above trained a model from scratch. The model is saved to the hard drive and can be used for further analysis.\n",
    "\n",
    "In chapter 9, we learned how to use such a model, so please take a look at that code in order to see how to use this model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
