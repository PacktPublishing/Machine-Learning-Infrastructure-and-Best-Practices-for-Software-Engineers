{"cells":[{"cell_type":"markdown","metadata":{"id":"s_q15InjLhCs"},"source":["# Using YoLo v5 network\n","\n","In this workbook, we use YoLo (You only Look once) network from Hugging Face to detect objects. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISzNXM20QV2S"},"outputs":[],"source":["!pip install --upgrade numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZ1U_40iWuQK"},"outputs":[],"source":["# install YoLo v5 network\n","!pip install -q -U yolov5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":370,"output_embedded_package_id":"1U2cWDsnVpfhQE_w3MZ8bFvwiddrFHIv1"},"executionInfo":{"elapsed":1176,"status":"error","timestamp":1684572881256,"user":{"displayName":"Miroslaw Staron","userId":"03361447941535209117"},"user_tz":-120},"id":"6Z8nyUniW3cb","outputId":"baeed547-b3f7-46cd-b74d-f1cfea2a2891"},"outputs":[],"source":["import yolov5\n","\n","# load model\n","model = yolov5.load('fcakyon/yolov5s-v7.0')\n","  \n","# set model parameters\n","model.conf = 0.25  # NMS confidence threshold\n","model.iou = 0.45  # NMS IoU threshold\n","model.agnostic = False  # NMS class-agnostic\n","model.multi_label = False  # NMS multiple labels per box\n","model.max_det = 1000  # maximum number of detections per image\n","\n","# set image\n","img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n","\n","# Load and preprocess the image\n","image = Image.open('./test_image.jpg')\n","transform = transforms.Compose([transforms.Resize((640, 640)),\n","                                transforms.ToTensor()])\n","image = transform(image)\n","\n","\n","# perform inference\n","results = model(image)\n","\n","# inference with larger input size\n","results = model(image, size=640)\n","\n","# inference with test time augmentation\n","results = model(image, augment=True)\n","\n","# parse results\n","predictions = results.pred[0]\n","boxes = predictions[:, :4] # x1, y1, x2, y2\n","scores = predictions[:, 4]\n","categories = predictions[:, 5]\n","\n","# show detection bounding boxes on image\n","results.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":614},"executionInfo":{"elapsed":8399,"status":"error","timestamp":1684572577098,"user":{"displayName":"Miroslaw Staron","userId":"03361447941535209117"},"user_tz":-120},"id":"Nqmq1uYqV4sS","outputId":"76da5c04-468a-4f9f-847e-a3384059eea4"},"outputs":[],"source":["import torch\n","from torchvision import transforms\n","from PIL import Image\n","\n","# Load the model from Hugging Face\n","model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Load and preprocess the image\n","image = Image.open('./test_image.jpg')\n","transform = transforms.Compose([transforms.Resize((640, 640)),\n","                                transforms.ToTensor()])\n","image = transform(image)\n","\n","# Perform object detection\n","results = model(image)\n","\n","# Print the predicted labels and bounding boxes\n","for label, bbox in zip(results.xyxyn[0][:, -1].numpy(), results.xyxyn[0][:, :-1].numpy()):\n","    print(f'Label: {label}, Bounding Box: {bbox}')\n","\n","# Show the annotated image\n","results.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgt7n_-bMK8x"},"outputs":[],"source":["# read the file with data using openpyxl\n","import pandas as pd\n","\n","# we read the data from the excel file, \n","# which is the defect data from the ant 1.3 system\n","dfDataCamel12 = pd.read_excel('./chapter_6_dataset_numerical.xlsx', \n","                            sheet_name='camel_1_2',\n","                            index_col=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684487786153,"user":{"displayName":"Miroslaw Staron","userId":"03361447941535209117"},"user_tz":-120},"id":"824XQ9xSNBW8","outputId":"312689ac-86bd-4848-8658-f60daacdda6f"},"outputs":[],"source":["dfDataCamel12"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sbo7eXQlMWfe"},"outputs":[],"source":["# prepare the dataset\n","import sklearn.model_selection\n","\n","X = dfDataCamel12.drop(['Defect'], axis=1)\n","y = dfDataCamel12.Defect\n","\n","X_train, X_test, y_train, y_test = \\\n","        sklearn.model_selection.train_test_split(X, y, random_state=42, train_size=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261,"status":"ok","timestamp":1684487804653,"user":{"displayName":"Miroslaw Staron","userId":"03361447941535209117"},"user_tz":-120},"id":"exqSCqseSB2D","outputId":"0c761a34-8055-41c5-fa79-bde45b1a1d8f"},"outputs":[],"source":["y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bYa65S9NMWyF"},"outputs":[],"source":["import autosklearn.classification\n","cls = autosklearn.classification.AutoSklearnClassifier()\n","cls.fit(X_train, y_train)\n","predictions = cls.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BbcEGbRDW1Yx","outputId":"b24c7c50-d3c8-47bb-915b-9f8eff4d1170"},"outputs":[],"source":["# now we can get the best models with their weights\n","cls.leaderboard()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"969_OMr2XH2v","outputId":"bad20b97-64d8-472e-b5ee-76d9f40a6746"},"outputs":[],"source":["# and we can even take a look at the best models' scores\n","ensemble_dict = cls.show_models()\n","print(ensemble_dict)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPVfqbx6EwyjiqmgFA76/EA","mount_file_id":"1tM4hqJKJtZr_tdI9c74IGbNfqb2mU2gY","name":"","provenance":[{"file_id":"1Kvl9mEqrzhcpgDFqH0h4RsU8gd3rVyaO","timestamp":1684572343412}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
